---
title: "Lyra Progress — WebUI, vLLM, and Household Memory"
date: 2025-11-20
tags:
  - lyra
  - progress-journal
  - vllm
  - qdrant
  - webui
  - local-llm
---

# 20 November, 2025 — Lyra Web Interface & Memory Integration

## Context

This session focused on giving Lyra a functional interim web interface and establishing persistent, structured memory accessible during inference. The primary motivation was to stabilize Lyra’s interaction surface while constructing a foundation for long term memory using Qdrant. The eventual intention is that her responses reflect real, household specific knowledge rather than model guesswork. The work revolved around integrating Open-WebUI, correcting gateway routing, and building a cohesive RAG pipeline grounded in a YAML-defined world model.

Today we achieved routing Open-WebUI through the vLLM gateway. This is a temporary gui for testing interaction while the final web interface for the mirror is developed. 
I wired in the first attempt at household memory via Qdrant, using `house_profile.yaml` as seed knowledge. The yaml is ingested and the difference is vectorized and added to the database. 
This results in the chain: browser → Open-WebUI → vLLM → Lyra persona wrapper → Qdrant retrieval → answer

The main goal wasn't fast retrieval for fast retrieval's sake, it was to get a stable, explainable path from a question over the LAN to a grounded answer that knows its host hardware and details of the household in which it operates. Fast retrieval is an added bonus, as this is just one component of a communications stack that is intended to have a conversational response time.

---

## Work

### 1. Web Interface & Gateway Routing

We started by making sure Open-WebUI talks to vLLM instead of the default backend.

Key checks:

```bash
# See which vLLM containers are actually running
docker ps | grep vllm

# Example expected container
# vllm-qwen7b   vllm/vllm-openai:latest   "python3 -m vllm.entrypoints.openai.api_server ..."   0.0.0.0:8001->8000/tcp
```

Then we validated Open-WebUI’s configuration so that:

- `OPENAI_API_BASE` points at the vLLM endpoint (`http://lyra:8001/v1`).
- `OPENAI_API_KEY` is set to a local dummy token, since vLLM doesn’t care what the key actually is.
- The model name in WebUI (today we are running qwen2.5-7b-instruct) matches what vLLM advertises.

docker-compose.yml snippet for vLLM:

```yaml
services:
  vllm-qwen7b:
    image: vllm/vllm-openai:latest
    command:
      - "python3"
      - "-m"
      - "vllm.entrypoints.openai.api_server"
      - "--model"
      - "Qwen/Qwen2.5-7B-Instruct"
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "8000"
      - "--dtype"
      - "bfloat16"
    ports:
      - "8001:8000"
    environment:
      NVIDIA_VISIBLE_DEVICES: "0" 
    volumes:
      - /mnt/data1/models:/models
    runtime: nvidia
```

A quick curl test confirmed the gateway was alive:

```bash
curl http://lyra:8001/v1/models | jq
```

This returned a JSON list where `id` corresponds to the Qwen 7B model. When that showed up, we knew Open-WebUI had the model.

---

### 2. Persona Wrapper & Logical Flow

On top of the raw vLLM endpoint, Lyra uses a small Python persona wrapper that accepts endpoint requests and injects Lyra’s system prompt and identity. It calls into Qdrant to retrieve household context and streams the final answer back using Chat Completions API format.


```python
from fastapi import FastAPI
from openai import OpenAI
from qdrant_client import QdrantClient
from main import build_lyra_system_prompt, retrieve_household_context

app = FastAPI()
llm_client = OpenAI(base_url="http://lyra:8001/v1", api_key="not-used")
qdrant = QdrantClient(host="lyra", port=6333)

@app.post("/v1/chat/completions")
async def chat_completions(request: dict):
    user_message = request["messages"][-1]["content"]
    retrieval = retrieve_household_context(user_message, qdrant=qdrant)

    system_prompt = build_lyra_system_prompt(extra_context=retrieval)

    completion = llm_client.chat.completions.create(
        model="qwen2.5-7b-instruct",
        messages=[
            {"role": "system", "content": system_prompt},
            *request["messages"],
        ],
        stream=request.get("stream", False),
    )
    return completion
```

The logic here allows for Lyra to always be framed with a stable system prompt (persona, safety constraints, memory policy). Retrieval is additive knowledge, not a full script of what to say.
The user’s original message is kept intact, so the model sees both what the human asked and what Lyra already knows about them.

---

### 3. Household Memory Ingestion (Qdrant)

I then wired in a first-pass ingestion pipeline from `house_profile.yaml` to Qdrant. The YAML describes things like residents and their roles in the household, hardware layout (Lyra GPU box, mirror, Pi nodes), and network basics and “what lives where” in each room.

Ingestion script:

```python
import yaml
from qdrant_client import QdrantClient
from qdrant_client.models import PointStruct
from sentence_transformers import SentenceTransformer
import uuid

qdrant = QdrantClient(host="lyra", port=6333)
embedder = SentenceTransformer("all-MiniLM-L6-v2")

with open("house_profile.yaml", "r") as f:
    profile = yaml.safe_load(f)

chunks = []

for section_name, section_data in profile.items():
    text = yaml.dump(section_data, sort_keys=False)
    chunks.append({
        "id": str(uuid.uuid4()),
        "payload": {"section": section_name, "text": text},
        "text": text,
    })

vectors = embedder.encode([c["text"] for c in chunks])

points = [
    PointStruct(
        id=c["id"],
        payload=c["payload"],
        vector=v.tolist()
    )
    for c, v in zip(chunks, vectors)
]

qdrant.recreate_collection(
    collection_name="house_docs",
    vectors_config={"size": len(vectors[0]), "distance": "Cosine"}
)

qdrant.upsert(collection_name="house_docs", points=points)
print(f"Ingested {len(points)} household memory chunks.")
```

Retrieval helper in `main.py`:

```python
def retrieve_household_context(query: str, qdrant: QdrantClient, limit: int = 5) -> str:
    embedding = embedder.encode([query])[0].tolist()
    hits = qdrant.search(
        collection_name="house_docs",
        vector=embedding,
        limit=limit,
        with_payload=True,
    )
    if not hits:
        return ""

    snippets = []
    for hit in hits:
        section = hit.payload.get("section", "unknown")
        text = hit.payload.get("text", "")
        snippets.append(f"[{section}] {text}")

    return "\n\n".join(snippets)
```

I was able to validate ingestion with a scroll:

```bash
curl -X POST "http://127.0.0.1:6333/collections/house_docs/points/scroll"   -H "Content-Type: application/json"   -d '{"limit": 3}' | jq
```

Seeing `section` and `text` payloads confirmed that household memory exists as structured, searchable data rather than vibes. The Qdrant web gui reflected the new objects.

---

### 4. Ingestion Workflow

`house_profile.yaml` is ingested by `ingest_house_profile.py`. That script generates embeddings using FastEmbed. Embeddings and metadata are stored in the `house_docs` collection in Qdrant.
At query time, Lyra takes the user’s message, calls `retrieve_household_context()` against Qdrant, pulls top-k similar memories, and wraps those into the persona prompt.

---

### 5. Model/Runtime Topology

Open-WebUI runs as the temporary chat front-end. A gateway service exposes a local “OpenAI-style” API. vLLM runs as the main inference engine. Qdrant holds long-term household memory (`house_docs`, etc.).

Requests flow from browser → Open-WebUI → Gateway → vLLM + Qdrant → Response back to browser.

---

## Observations

### Behavior Changes After Retrieval

Before Qdrant was wired in, Lyra could talk about “your GPU” in generic terms, but it had no grounded knowledge of the actual build. After ingestion, when asked “Tell me about your hardware in detail”, the answer referenced the Ryzen CPU, RTX 5090 GPU, and the storage layout. The tone shifted from speculative to descriptive. It no longer guessed model numbers based on the question, it recalled them from database memory.
Qualitatively, this results in less hallucinated detail and more continuity between sessions: the same residents, the same hardware, the same roles.

### Performance & GPU Utilization

Qwen 7B under vLLM on the 5090 is extremely fast for single-user, LAN-bound chat. Token generation felt instant enough for interactive use. vLLM’s paged attention meant the context window could be large without choking VRAM. GPU memory is the limiting factor for larger models, but not for running 7B at high throughput.
The important discovery: 7B as my intended lighter weight model does not feel like a sad fallback; with retrieval and a strong persona wrapper, it behaves like a much larger system because it’s anchored in real data.

---

## Next Steps

1. **Memory Expansion Beyond `house_profile.yaml`**  
   Add more project documents (governance policies, risk sim notes, Echo build logs) into additional Qdrant collections. Tell Lyra what PSU she has again because that edit got overwritten while I was refining the ingestion script. 
   Add more metadata fields (doc_type, created_at, sensitivity) for smarter retrieval and future governance rules.

2. **Persona Refinement & Safety Rails**  
   Add more of the AI Constitution and memory ethics into `build_lyra_system_prompt()`.
   Separate “factual household context” from “emotional/supportive tone” so Lyra can be both warm and precise.

3. **Monitoring & Telemetry**  
   Add basic metrics for Qdrant search latency, vLLM token throughput per request, and GPU VRAM utilization over time.
   Store these in Prometheus/Grafana so I can correlate “Lyra feels slow today” with actual resource graphs.

4. **Prepare for Mirror Integration**  
  Define a small, stable HTTP API that the Magic Mirror front-end can hit for chat completions, and integrate it into the web interface design.
  Quick status pings (is vLLM up, is Qdrant up). 
  Echo's physical build is coming along.. consistently. Two more mortises to cut for the bottom panel of the shadowbox, final touches on the paint, and I can start wiring in the components. 
  
5. **VRAM & Second Node Planning**  
  Treat the 5090 + 7B vLLM stack as the inference workhorse for now while planning the text to speech layer. 14b is a likely next step if it's possible for it to play well with speech.
  Begin designing a second node focused on TTS/STT and possibly larger quantized experimentation models, keeping this journal entry as the baseline of what currently works well. I'm not convinced that a 14b model and Parler TTS will work together to produce my intended results but quant schema and context length variables might prove me wrong. 

This week is when Lyra stopped being just a fast toy model and gained context, temporal persistence, and the accompanied early programmatic governance. The retrieval gateway and storage layers will now be the basis for additional deveopment and theory.
